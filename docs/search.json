[
  {
    "objectID": "ampliseq.html",
    "href": "ampliseq.html",
    "title": "Ampliseq pipeline",
    "section": "",
    "text": "Terminal multiplexers allow you to run multiple terminal sessions in a single terminal window. This is useful for running long jobs that you don’t want to be interrupted if you lose your connection to the server. The two most popular terminal multiplexers are tmux and screen.\n\n\ntmux\nA tmux ‘cheat sheet’ can be found here.\n\n\n\nscreen\nScreen ‘cheat sheets’ can be found here and here."
  },
  {
    "objectID": "ampliseq.html#use-a-terminal-multiplexer",
    "href": "ampliseq.html#use-a-terminal-multiplexer",
    "title": "Ampliseq pipeline",
    "section": "",
    "text": "Terminal multiplexers allow you to run multiple terminal sessions in a single terminal window. This is useful for running long jobs that you don’t want to be interrupted if you lose your connection to the server. The two most popular terminal multiplexers are tmux and screen.\n\n\ntmux\nA tmux ‘cheat sheet’ can be found here.\n\n\n\nscreen\nScreen ‘cheat sheets’ can be found here and here."
  },
  {
    "objectID": "ampliseq.html#setting-up-the-ampliseq-pipeline",
    "href": "ampliseq.html#setting-up-the-ampliseq-pipeline",
    "title": "Ampliseq pipeline",
    "section": "Setting up the ampliseq pipeline",
    "text": "Setting up the ampliseq pipeline\n\nLoad the nextflow module and the apptainer module\n\nmodule load nextflow\nmodule load PDC apptainer\n\nSet environment variables\n\nexport NXF_HOME=\"&lt;/path/to/where/you/will/run/ampliseq&gt;\"\nexport APPTAINER_BINDPATH=$PDC_TMP\nexport APPTAINER_CACHEDIR=$PDC_TMP/.cache/apptainer\nNote that you will also have to create the $PDC_TMP/.cache/apptainer directory once. You can do so with:\nmkdir -p $PDC_TMP/.cache/apptainer\n\nRun pipeline\n\nThe general syntax to running a Nextflow pipeline is:\nnextflow run &lt;pipeline&gt; -r &lt;release&gt; -profile &lt;profile&gt; --project &lt;project&gt; --input &lt;input&gt; --outdir &lt;outdir&gt;\nArgument flags that begin with a single ‘-’ are Nextflow flags, while those that begin with a double ‘–’ are pipeline-specific flags.\nThe -r flag is used to select a specific release of the pipeline. To see available releases, visit the pipeline’s GitHub page at https://github.com/nf-core/ampliseq/releases. At the time of writing, the latest release is 2.12.0.\nThe -profile flag is used to select a configuration profile for running the pipeline. This can be used to alter the resources allocated to the pipeline, among other things. The Dardel PDC profile for nf-core is available at https://nf-co.re/configs/pdc_kth and can be specified with --profile pdc_kth.\nThe --project flag is used to specify a compute account on a cluster.\nThe --input flag is used to specify the path to a sample sheet file. This file should be in TSV format and should contain the following columns:\n\nsampleID (required): Unique sample IDs, must start with a letter, and can only contain letters, numbers or underscores\nforwardReads (required): Paths to (forward) reads zipped FastQ files\nreverseReads (optional): Paths to reverse reads zipped FastQ files, required if the data is paired-end\nrun (optional): If the data was produced by multiple sequencing runs, any string\n\nThe --outdir flag is used to specify the path to the output directory.\nAn Ampliseq-compatible sample sheet is generated by the amplicon-multi-cutadapt pipeline.\nSeveral steps in the ampliseq pipeline are not needed if the data will be run through the HAPP pipeline. For instance:\n\n--skip_cutadapt skips the cutadapt step\n--skip_qiime skips the QIIME2 step\n--skip_taxonomy skips the taxonomy step\n--skip_dada_taxonomy skips the dada taxonomy step\n--skip_dada_addspecies skips the dada species assignment step\n\nIf there are samples with too few reads (default threshold is 1) you can set the pipeline to ignore those samples by using the --ignore_empty_input_files and --ignore_failed_trimming"
  },
  {
    "objectID": "ampliseq.html#example-run",
    "href": "ampliseq.html#example-run",
    "title": "Ampliseq pipeline",
    "section": "Example run",
    "text": "Example run\nnextflow run nf-core/ampliseq -r 2.12.0 -profile pdc_kth --project naiss2024-5-207 \\\n    --input ampliseq_sample_sheet.tsv --outdir results \\\n    --skip_cutadapt --skip_qiime --skip_taxonomy --skip_dada_taxonomy --skip_dada_addspecies --ignore_failed_trimming --ignore_empty_input_files"
  },
  {
    "objectID": "ampliseq.html#further-configuration",
    "href": "ampliseq.html#further-configuration",
    "title": "Ampliseq pipeline",
    "section": "Further configuration",
    "text": "Further configuration\nThe PDC profile handles resources etc. for the pipeline on Dardel.\nSteps in the pipeline can be further configured by supplying an additional config file to nextflow. For instance, you can create a file called nextflow.config file and add the following (example for the FASTQC step:\nprocess {\n    withName: 'NFCORE_AMPLISEQ:AMPLISEQ:FASTQC' {\n        time = 10.m\n        cpus = 1\n        memory = 1.GB\n    }\n}\nThen supply the config file to the pipeline:\nnextflow run nf-core/ampliseq -r 2.12.0 -profile pdc_kth -c nextflow.config --project naiss2024-5-207 \\\n    --input ampliseq_sample_sheet.tsv --outdir results \\\n    --skip_cutadapt --skip_qiime --skip_taxonomy --skip_dada_taxonomy --skip_dada_addspecies --ignore_failed_trimming --ignore_empty_input_files"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Insect Biome Atlas",
    "section": "",
    "text": "Documentation for the Insect Biome Atlas project."
  },
  {
    "objectID": "workflows.html",
    "href": "workflows.html",
    "title": "Running workflows on Dardel",
    "section": "",
    "text": "Workflows are centrally installed under /cfs/klemming/projects/snic/snic2020-16-248/workflows and are managed by the package snk which turns snakemake workflows into command-line tools. Thus instead of doing cloning workflows from GitHub into some directory on Dardel, then executing snakemake ... within the cloned directory you activate an environment from the root of the storage project (/cfs/klemming/projects/snic/snic2020-16-248) then run e.g. amplicon-multi-cutadapt ... in whatever directory you want.\n\n\nA requirement is that you have installed pixi on Dardel (check if pixi --version works). If you haven’t installed pixi you can do so by running:\ncurl -fsSL https://pixi.sh/install.sh | bash\nThis part is only necessary if you haven’t installed pixi before.\n\n\n\nThe file pixi.toml located in the root of the storage project (/cfs/klemming/projects/snic/snic2020-16-248) defines environments that are ready to use with the workflows amplicon-multi-cutadapt and and happ.\nThe general syntax for activating one of the workflow environments is to run the following from the project root directory (/cfs/klemming/projects/snic/snic2020-16-248):\npixi shell -e &lt;workflow-name&gt;\nor from any directory by pointing to the pixi.toml file:\npixi shell -e &lt;workflow-name&gt; --manifest-path /cfs/klemming/projects/snic/snic2020-16-248/pixi.toml\nSo to activate the amplicon-multi-cutadapt environment you would run:\npixi shell -e amplicon-multi-cutadapt /cfs/klemming/projects/snic/snic2020-16-248/pixi.toml\nYou could then run:\namplicon-multi-cutadapt -h\nTo see a help message for the workflow.\n\n\n\nFor the Snakemake workflows (happ/amplicon-multi-cutadapt) their respective environment is set up to use a Dardel-specific configuration profile which sets the compute account and other resources. The path to this profile is set in the $SNAKEMAKE_PROFILE environment variable. If you need to edit for example the compute account you can open the file $SNAKEMAKE_PROFILE/config.yaml with a text editor and change the slurm_account: setting, then save the file.\nFor the ampliseq workflow resources are handled slightly differently. See below.\n\n\n\n\nWhen you activate an environment for one of the two Snakemake workflows (amplicon-multi-cutadapt and happ), the workflows become available as command line tools. This means you don’t have to interact with the workflow via Snakemake and can skip several of the additional flags.\nLet’s use the amplicon-multi-cutadapt workflow as an example. After activating the environment as described above, you can run the workflow with:\namplicon-multi-cutadapt run --config &lt;path-to-configfile&gt;\nThe only thing you need to provide is the path to a configuration file that contains parameters specific to your run. Let’s say you want to run the workflow for a dataset called batch1. Create a subdirectory under projects/ called batch1 and navigate to it. Then create a configuration file with default settings by running:\namplicon-multi-cutadapt config &gt; config.yml\nNow you can edit the config.yml with the parameters specific to your run. When you’re done, you can run the workflow with:\namplicon-multi-cutadapt run --configfile config.yml\nThe same basic principle applies to the happ workflow. For more details, see the documentation for each of these workflows in the /cfs/klemming/projects/snic/snic2020-16-248/docs/ directory.\n\n\n\nThe ampliseq workflow differs slightly since this is a Nextflow workflow. When you activate the ampliseq environment you will have to start the workflow using nextflow run, but the path to workflow is stored in the $AMPLISEQ environment variable. To run the workflow, navigate to the project root directory and run:\n```bash"
  },
  {
    "objectID": "workflows.html#installing-pixi",
    "href": "workflows.html#installing-pixi",
    "title": "Running workflows on Dardel",
    "section": "",
    "text": "A requirement is that you have installed pixi on Dardel (check if pixi --version works). If you haven’t installed pixi you can do so by running:\ncurl -fsSL https://pixi.sh/install.sh | bash\nThis part is only necessary if you haven’t installed pixi before."
  },
  {
    "objectID": "workflows.html#how-to-activate-a-workflow-environment",
    "href": "workflows.html#how-to-activate-a-workflow-environment",
    "title": "Running workflows on Dardel",
    "section": "",
    "text": "The file pixi.toml located in the root of the storage project (/cfs/klemming/projects/snic/snic2020-16-248) defines environments that are ready to use with the workflows amplicon-multi-cutadapt and and happ.\nThe general syntax for activating one of the workflow environments is to run the following from the project root directory (/cfs/klemming/projects/snic/snic2020-16-248):\npixi shell -e &lt;workflow-name&gt;\nor from any directory by pointing to the pixi.toml file:\npixi shell -e &lt;workflow-name&gt; --manifest-path /cfs/klemming/projects/snic/snic2020-16-248/pixi.toml\nSo to activate the amplicon-multi-cutadapt environment you would run:\npixi shell -e amplicon-multi-cutadapt /cfs/klemming/projects/snic/snic2020-16-248/pixi.toml\nYou could then run:\namplicon-multi-cutadapt -h\nTo see a help message for the workflow."
  },
  {
    "objectID": "workflows.html#resource-settings-slurm-account-runtime-etc",
    "href": "workflows.html#resource-settings-slurm-account-runtime-etc",
    "title": "Running workflows on Dardel",
    "section": "",
    "text": "For the Snakemake workflows (happ/amplicon-multi-cutadapt) their respective environment is set up to use a Dardel-specific configuration profile which sets the compute account and other resources. The path to this profile is set in the $SNAKEMAKE_PROFILE environment variable. If you need to edit for example the compute account you can open the file $SNAKEMAKE_PROFILE/config.yaml with a text editor and change the slurm_account: setting, then save the file.\nFor the ampliseq workflow resources are handled slightly differently. See below.\n\n\n\n\nWhen you activate an environment for one of the two Snakemake workflows (amplicon-multi-cutadapt and happ), the workflows become available as command line tools. This means you don’t have to interact with the workflow via Snakemake and can skip several of the additional flags.\nLet’s use the amplicon-multi-cutadapt workflow as an example. After activating the environment as described above, you can run the workflow with:\namplicon-multi-cutadapt run --config &lt;path-to-configfile&gt;\nThe only thing you need to provide is the path to a configuration file that contains parameters specific to your run. Let’s say you want to run the workflow for a dataset called batch1. Create a subdirectory under projects/ called batch1 and navigate to it. Then create a configuration file with default settings by running:\namplicon-multi-cutadapt config &gt; config.yml\nNow you can edit the config.yml with the parameters specific to your run. When you’re done, you can run the workflow with:\namplicon-multi-cutadapt run --configfile config.yml\nThe same basic principle applies to the happ workflow. For more details, see the documentation for each of these workflows in the /cfs/klemming/projects/snic/snic2020-16-248/docs/ directory.\n\n\n\nThe ampliseq workflow differs slightly since this is a Nextflow workflow. When you activate the ampliseq environment you will have to start the workflow using nextflow run, but the path to workflow is stored in the $AMPLISEQ environment variable. To run the workflow, navigate to the project root directory and run:\n```bash"
  },
  {
    "objectID": "happ.html",
    "href": "happ.html",
    "title": "HAPP pipeline",
    "section": "",
    "text": "Terminal multiplexers allow you to run multiple terminal sessions in a single terminal window. This is useful for running long jobs that you don’t want to be interrupted if you lose your connection to the server. The two most popular terminal multiplexers are tmux and screen.\n\n\ntmux\nA tmux ‘cheat sheet’ can be found here.\n\n\n\nscreen\nScreen ‘cheat sheets’ can be found here and here."
  },
  {
    "objectID": "happ.html#use-a-terminal-multiplexer",
    "href": "happ.html#use-a-terminal-multiplexer",
    "title": "HAPP pipeline",
    "section": "",
    "text": "Terminal multiplexers allow you to run multiple terminal sessions in a single terminal window. This is useful for running long jobs that you don’t want to be interrupted if you lose your connection to the server. The two most popular terminal multiplexers are tmux and screen.\n\n\ntmux\nA tmux ‘cheat sheet’ can be found here.\n\n\n\nscreen\nScreen ‘cheat sheets’ can be found here and here."
  },
  {
    "objectID": "happ.html#setting-up-the-happ-pipeline",
    "href": "happ.html#setting-up-the-happ-pipeline",
    "title": "HAPP pipeline",
    "section": "Setting up the HAPP pipeline",
    "text": "Setting up the HAPP pipeline\n\nInstallation\nInstall the HAPP pipeline by cloning the repository somewhere on Dardel:\ngit clone git@github.com:insect-biome-atlas/happ.git\nthis will create a directory called happ in your current working directory. Change into the happ directory with cd happ.\n\n\nChoosing a branch or release\nBy default the pipeline will use the latest commit from the main branch. To instead use the devel branch, run:\ngit switch devel\nTo use a specific release, switch to the corresponding tag (you can see available tags at https://github.com/insect-biome-atlas/happ/tags). For example, to use release v2.1:\ngit switch --detach v2.1\n\n\nSet up the environment\nThe software required for running the workflow can be installed with either pixi or conda.\nTo install and activate the software environment with pixi, run:\npixi shell\nTo install with conda, run:\nmkdir envs\nconda env create -f environment.yml -p envs/happ\nThis will create a conda environment called happ in the envs directory. Activate the environment with:\nconda activate envs/happ\nNext load the apptainer module:\nmodule load PDC apptainer"
  },
  {
    "objectID": "happ.html#configuring",
    "href": "happ.html#configuring",
    "title": "HAPP pipeline",
    "section": "Configuring",
    "text": "Configuring\nThe default configuration of the pipeline is found in file config/config.yml. To configure the pipeline for your specific needs, copy this file to a new file inside the config/ subdirectory, e.g. config/config_myproject.yml. Edit this file to suit your needs."
  },
  {
    "objectID": "happ.html#input",
    "href": "happ.html#input",
    "title": "HAPP pipeline",
    "section": "Input",
    "text": "Input\nThe pipeline requires two input files as minimum:\n\nA fasta file with ASV sequences, and\nA file with counts of each ASV in each sample.\n\nBoth files must be tab-separated with the first column containing the ASV ids. Place these files in a subfolder of the data/ directory, e.g. data/myproject/ and name them asv_seqs.fasta and asv_counts.tsv, respectively.\nThen edit the rundir parameter of your configuration file so that it matches the name of the subdirectory under data/ where you placed the input files (in the example above it would be myproject)."
  }
]