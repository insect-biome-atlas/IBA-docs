[
  {
    "objectID": "dardel.html",
    "href": "dardel.html",
    "title": "Dardel tips",
    "section": "",
    "text": "No backup\n\n\n\nThere is no backup on Dardel. All data that cannot be easily recreated should therefore have an alternative backup.",
    "crumbs": [
      "Dardel system"
    ]
  },
  {
    "objectID": "dardel.html#temporary-directory",
    "href": "dardel.html#temporary-directory",
    "title": "Dardel tips",
    "section": "Temporary directory",
    "text": "Temporary directory\nThe environment variable $PDC_TMP points to your personal scratch space under /cfs/klemming/scratch. This scratch space has no limitation in terms of storage, but files not updated in 30 days will be automatically removed. Therefore it’s a great place to store temporary downloads, cache directories etc.",
    "crumbs": [
      "Dardel system"
    ]
  },
  {
    "objectID": "dardel.html#setting-up-ssh-keys",
    "href": "dardel.html#setting-up-ssh-keys",
    "title": "Dardel tips",
    "section": "Setting up SSH keys",
    "text": "Setting up SSH keys\nTo access the Dardel cluster, you need to generate a SSH key on the computer you will be connecting from (e.g. your laptop). On Linux/Mac, open a terminal and run:\nssh-keygen -t ed25519 -f ~/.ssh/id-ed25519-pdc\nYou will see the following:\nGenerating public/private ed25519 key pair.\nEnter passphrase (empty for no passphrase):\nJust press Enter (and once again again to confirm) to leave the passphrase empty. You will now have two files in your ~/.ssh directory: id-ed25519-pdc (private key) and id-ed25519-pdc.pub (public key).\nYou will now have to register the public key at PDC by going go https://loginportal.pdc.kth.se/ and logging in. On the “Key Management” page, click “Add new key” and either upload the id-ed25519-pdc.pub file or paste its contents in the “SSH public key” field. Give the key a name (can be anything) and click Save.\nYou can now connect to Dardel by running (replace &lt;your-dardel-username&gt; with your Dardel username):\nssh -i ~/.ssh/id-ed25519-pdc &lt;your-dardel-username&gt;@dardel.pdc.kth.se\nTo shorten that command you can create a file called config inside your ~/.ssh directory with the following content (replace &lt;your-dardel-username&gt; with your Dardel username):\nHost dardel\n   IdentityFile ~/.ssh/id-ed25519-pdc\n   HostName dardel.pdc.kth.se\n   User &lt;your-dardel-username&gt;\nThen you connect to Dardel by running:\nssh dardel",
    "crumbs": [
      "Dardel system"
    ]
  },
  {
    "objectID": "dardel.html#setting-up-ssh-keys-for-github",
    "href": "dardel.html#setting-up-ssh-keys-for-github",
    "title": "Dardel tips",
    "section": "Setting up ssh keys for GitHub",
    "text": "Setting up ssh keys for GitHub\nIf you want to clone repositories from GitHub on Dardel, you need to generate a key on Dardel and add it to your GitHub account. On Dardel, run (replace your_email@example.com with the email associated with your GitHub account):\nssh-keygen -t ed25519 -C \"your_email@example.com\"\nThis will generate a private key in ~/.ssh/id_ed25519 and a public key in ~/.ssh/id_ed25519.pub. You can now add the public key to your GitHub account. Go to https://github.com/settings/keys (login if necessary) and click “New SSH key”. Paste the contents of ~/.ssh/id_ed25519.pub into the “Key” field and give the key a name (can be anything). Click “Add SSH key”.\nOn Dardel, edit (or create) the ~/.ssh/config file and add the following:\nHost github.com\n    AddKeysToAgent yes\n    IdentityFile ~/.ssh/id_ed25519",
    "crumbs": [
      "Dardel system"
    ]
  },
  {
    "objectID": "dardel.html#use-a-terminal-multiplexer",
    "href": "dardel.html#use-a-terminal-multiplexer",
    "title": "Dardel tips",
    "section": "Use a terminal multiplexer",
    "text": "Use a terminal multiplexer\nTerminal multiplexers allow you to run multiple terminal sessions in a single terminal window. This is useful for running long jobs that you don’t want to be interrupted if you lose your connection to the server. The two most popular terminal multiplexers are tmux and screen.\n\nUsing tmux\ntmux\nA tmux ‘cheat sheet’ can be found here.\n\n\nUsing screen\nscreen\nScreen ‘cheat sheets’ can be found here and here.",
    "crumbs": [
      "Dardel system"
    ]
  },
  {
    "objectID": "dardel.html#directories-and-files-in-this-project-root-folder",
    "href": "dardel.html#directories-and-files-in-this-project-root-folder",
    "title": "Dardel tips",
    "section": "Directories and files in this project root folder",
    "text": "Directories and files in this project root folder\ndb/ # Databases\ndocs/ # Documentation of pipelines and other useful information\ngit/ # Cloned repositories from GitHub (as well as copies of folders from Rackham and results)\nprocessed_data/ # Processed data (e.g. amplicon datasets downloaded from Figshare)\nprojects/ # Main directory for all your projects\nraw_data/ # Raw data (e.g. fastq files)",
    "crumbs": [
      "Dardel system"
    ]
  },
  {
    "objectID": "dardel.html#directory-structure",
    "href": "dardel.html#directory-structure",
    "title": "Dardel tips",
    "section": "Directory structure",
    "text": "Directory structure\nThe following is a suggested directory structure for your work on Dardel. The projects/ directory could contain subdirectories for analysis of each new sequencing batch. The actual raw data for the sequencing should be stored under the raw_data/ directory (and this data should also have a backup outside of Dardel). Within each batch subdirectory you could create directories amplicon-multi-cutadapt and ampliseq for the output of the respective pipelines.\nTaking the P30904 batch as an example, the directory structure could look like this:\nprojects/\n  ├── P30904/\n  │   ├── amplicon-multi-cutadapt/\n  │   │   ├── results/\n  │   │   │   ├── cutadapt/\n  │   │   │   │   ├── P30904*_s4_R*.fastq.gz # Trimmed fastq files for each sample\n  │   │   ├── ampliseq_sample_sheet.tsv # Sample sheet for Ampliseq pipeline\n  │   ├── ampliseq/\n  │   │   ├── results/\n  │   │     ├── codon_filter/\n  │   │         ├── ASV_codon_filtered.fna # ASV sequences after dada2, length filtering and codon filtering\n  │   │         ├── ASV_codon_filtered.table.tsv # Counts for filtered ASVs",
    "crumbs": [
      "Dardel system"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Insect Biome Atlas",
    "section": "",
    "text": "Documentation for the Insect Biome Atlas project.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "workflows.html",
    "href": "workflows.html",
    "title": "Running workflows on Dardel",
    "section": "",
    "text": "Workflows are centrally installed under /cfs/klemming/projects/snic/snic2020-16-248/workflows and are managed by the package snk which turns snakemake workflows into command-line tools. Thus instead of doing cloning workflows from GitHub into some directory on Dardel, then executing snakemake ... within the cloned directory you activate an environment from the root of the storage project (/cfs/klemming/projects/snic/snic2020-16-248) then run e.g. amplicon-multi-cutadapt ... in whatever directory you want.",
    "crumbs": [
      "Workflows"
    ]
  },
  {
    "objectID": "workflows.html#installing-pixi",
    "href": "workflows.html#installing-pixi",
    "title": "Running workflows on Dardel",
    "section": "Installing pixi",
    "text": "Installing pixi\nA requirement is that you have installed pixi on Dardel (check if pixi --version works). If you haven’t installed pixi you can do so by running:\ncurl -fsSL https://pixi.sh/install.sh | bash\nThis part is only necessary if you haven’t installed pixi before.",
    "crumbs": [
      "Workflows"
    ]
  },
  {
    "objectID": "workflows.html#how-to-activate-a-workflow-environment",
    "href": "workflows.html#how-to-activate-a-workflow-environment",
    "title": "Running workflows on Dardel",
    "section": "How to activate a workflow environment",
    "text": "How to activate a workflow environment\nThe file pixi.toml located in the root of the storage project (/cfs/klemming/projects/snic/snic2020-16-248) defines environments that are ready to use with the workflows amplicon-multi-cutadapt and and happ.\nThe general syntax for activating one of the workflow environments is to run the following from the project root directory (/cfs/klemming/projects/snic/snic2020-16-248):\npixi shell -e &lt;workflow-name&gt;\nor from any directory by pointing to the pixi.toml file:\npixi shell -e &lt;workflow-name&gt; --manifest-path /cfs/klemming/projects/snic/snic2020-16-248/pixi.toml\nSo to activate the amplicon-multi-cutadapt environment you would run:\npixi shell -e amplicon-multi-cutadapt /cfs/klemming/projects/snic/snic2020-16-248/pixi.toml\nYou could then run:\namplicon-multi-cutadapt -h\nTo see a help message for the workflow.",
    "crumbs": [
      "Workflows"
    ]
  },
  {
    "objectID": "workflows.html#resource-settings-slurm-account-runtime-etc",
    "href": "workflows.html#resource-settings-slurm-account-runtime-etc",
    "title": "Running workflows on Dardel",
    "section": "Resource settings (slurm account, runtime etc)",
    "text": "Resource settings (slurm account, runtime etc)\nFor the Snakemake workflows (happ/amplicon-multi-cutadapt) their respective environment is set up to use a Dardel-specific configuration profile which sets the compute account and other resources. The path to this profile is set in the $SNAKEMAKE_PROFILE environment variable. If you need to edit for example the compute account you can open the file $SNAKEMAKE_PROFILE/config.yaml with a text editor and change the slurm_account: setting, then save the file.\nFor the ampliseq workflow resources are handled slightly differently. See below.\n\nRunning a workflow\n\nRunning the amplicon-multi-cutadapt and happ workflows\nWhen you activate an environment for one of the two Snakemake workflows (amplicon-multi-cutadapt and happ), the workflows become available as command line tools. This means you don’t have to interact with the workflow via Snakemake and can skip several of the additional flags.\nLet’s use the amplicon-multi-cutadapt workflow as an example. After activating the environment as described above, you can run the workflow with:\namplicon-multi-cutadapt run --config &lt;path-to-configfile&gt;\nThe only thing you need to provide is the path to a configuration file that contains parameters specific to your run. Let’s say you want to run the workflow for a dataset called batch1. Create a subdirectory under projects/ called batch1 and navigate to it. Then create a configuration file with default settings by running:\namplicon-multi-cutadapt config &gt; config.yml\nNow you can edit the config.yml with the parameters specific to your run. When you’re done, you can run the workflow with:\namplicon-multi-cutadapt run --configfile config.yml\nThe same basic principle applies to the happ workflow. For more details, see the documentation for each of these workflows in the /cfs/klemming/projects/snic/snic2020-16-248/docs/ directory.\n\n\nRunning the ampliseq workflow\nThe ampliseq workflow differs slightly since this is a Nextflow workflow. When you activate the ampliseq environment you will have to start the workflow using nextflow run, but the path to workflow is stored in the $AMPLISEQ environment variable. To run the workflow, navigate to the project root directory and run:\n```bash",
    "crumbs": [
      "Workflows"
    ]
  },
  {
    "objectID": "happ.html",
    "href": "happ.html",
    "title": "HAPP pipeline",
    "section": "",
    "text": "Note\n\n\n\nAs mentioned in the Workflows section you should first install Pixi to be able to use the centrally installed workflows for this project.\n\n\n\nActivate the pixi happ environment:\n\npixi shell -e happ --manifest-path /cfs/klemming/projects/snic/snic2020-16-248/pixi.toml\nAll necessary software (including Apptainer) should now be available.\n\nCreate and navigate to a directory where you want to run HAPP, e.g.:\n\nmkdir -p projects/myproject\ncd projects/myproject\n\nCreate a default configuration file\n\nhapp config &gt; config.yml\n\nMake any edits you need in config.yml (see Configuring below).\nRun the workflow:\n\n\n\n\n\n\n\nDid you remember the terminal multiplexer?\n\n\n\nTo avoid interruptions due to connection failure to Dardel it is recommended to run a terminal multiplexer on the server. See here for more info.\n\n\nhapp run --config config.yml --sdm=apptainer\n\n\n\nIf you prefer to get the HAPP pipeline directly from GitHub you should run:\ngit clone git@github.com:insect-biome-atlas/happ.git\nthis will create a directory called happ in your current working directory. Change into the happ directory with cd happ.\n\n\nBy default the pipeline will use the latest commit from the main branch. To instead use the devel branch, run:\ngit switch devel\nTo use a specific release, switch to the corresponding tag (you can see available tags at https://github.com/insect-biome-atlas/happ/tags). For example, to use release v2.1:\ngit switch --detach v2.1\n\n\n\nThe software required for running the workflow can be installed with either pixi or conda.\nTo install and activate the software environment with pixi, run:\npixi shell\nTo install with conda, run:\nmkdir envs\nconda env create -f environment.yml -p envs/happ\nThis will create a conda environment called happ in the envs directory. Activate the environment with:\nconda activate envs/happ\nNext load the apptainer module:\nmodule load PDC apptainer",
    "crumbs": [
      "Workflows",
      "HAPP pipeline"
    ]
  },
  {
    "objectID": "happ.html#installation",
    "href": "happ.html#installation",
    "title": "HAPP pipeline",
    "section": "",
    "text": "Note\n\n\n\nAs mentioned in the Workflows section you should first install Pixi to be able to use the centrally installed workflows for this project.\n\n\n\nActivate the pixi happ environment:\n\npixi shell -e happ --manifest-path /cfs/klemming/projects/snic/snic2020-16-248/pixi.toml\nAll necessary software (including Apptainer) should now be available.\n\nCreate and navigate to a directory where you want to run HAPP, e.g.:\n\nmkdir -p projects/myproject\ncd projects/myproject\n\nCreate a default configuration file\n\nhapp config &gt; config.yml\n\nMake any edits you need in config.yml (see Configuring below).\nRun the workflow:\n\n\n\n\n\n\n\nDid you remember the terminal multiplexer?\n\n\n\nTo avoid interruptions due to connection failure to Dardel it is recommended to run a terminal multiplexer on the server. See here for more info.\n\n\nhapp run --config config.yml --sdm=apptainer\n\n\n\nIf you prefer to get the HAPP pipeline directly from GitHub you should run:\ngit clone git@github.com:insect-biome-atlas/happ.git\nthis will create a directory called happ in your current working directory. Change into the happ directory with cd happ.\n\n\nBy default the pipeline will use the latest commit from the main branch. To instead use the devel branch, run:\ngit switch devel\nTo use a specific release, switch to the corresponding tag (you can see available tags at https://github.com/insect-biome-atlas/happ/tags). For example, to use release v2.1:\ngit switch --detach v2.1\n\n\n\nThe software required for running the workflow can be installed with either pixi or conda.\nTo install and activate the software environment with pixi, run:\npixi shell\nTo install with conda, run:\nmkdir envs\nconda env create -f environment.yml -p envs/happ\nThis will create a conda environment called happ in the envs directory. Activate the environment with:\nconda activate envs/happ\nNext load the apptainer module:\nmodule load PDC apptainer",
    "crumbs": [
      "Workflows",
      "HAPP pipeline"
    ]
  },
  {
    "objectID": "happ.html#configuring",
    "href": "happ.html#configuring",
    "title": "HAPP pipeline",
    "section": "Configuring",
    "text": "Configuring",
    "crumbs": [
      "Workflows",
      "HAPP pipeline"
    ]
  },
  {
    "objectID": "happ.html#input",
    "href": "happ.html#input",
    "title": "HAPP pipeline",
    "section": "Input",
    "text": "Input\nThe pipeline requires two input files as minimum:\n\nA fasta file with ASV sequences, and\nA file with counts of each ASV in each sample.\n\nBoth files must be tab-separated with the first column containing the ASV ids. Place these files in a subfolder of the data/ directory, e.g. data/myproject/ and name them asv_seqs.fasta and asv_counts.tsv, respectively.\nThen edit the rundir parameter of your configuration file so that it matches the name of the subdirectory under data/ where you placed the input files (in the example above it would be myproject).",
    "crumbs": [
      "Workflows",
      "HAPP pipeline"
    ]
  },
  {
    "objectID": "ampliseq.html",
    "href": "ampliseq.html",
    "title": "Ampliseq pipeline",
    "section": "",
    "text": "Load the nextflow module and the apptainer module\n\nmodule load nextflow\nmodule load PDC apptainer\n\nSet environment variables\n\nexport NXF_HOME=\"&lt;/path/to/where/you/will/run/ampliseq&gt;\"\nexport APPTAINER_BINDPATH=$PDC_TMP\nexport APPTAINER_CACHEDIR=$PDC_TMP/.cache/apptainer\nNote that you will also have to create the $PDC_TMP/.cache/apptainer directory once. You can do so with:\nmkdir -p $PDC_TMP/.cache/apptainer\n\nRun pipeline\n\nThe general syntax to running a Nextflow pipeline is:\nnextflow run &lt;pipeline&gt; -r &lt;release&gt; -profile &lt;profile&gt; --project &lt;project&gt; --input &lt;input&gt; --outdir &lt;outdir&gt;\nArgument flags that begin with a single ‘-’ are Nextflow flags, while those that begin with a double ‘–’ are pipeline-specific flags.\nThe -r flag is used to select a specific release of the pipeline. To see available releases, visit the pipeline’s GitHub page at https://github.com/nf-core/ampliseq/releases. At the time of writing, the latest release is 2.12.0.\nThe -profile flag is used to select a configuration profile for running the pipeline. This can be used to alter the resources allocated to the pipeline, among other things. The Dardel PDC profile for nf-core is available at https://nf-co.re/configs/pdc_kth and can be specified with --profile pdc_kth.\nThe --project flag is used to specify a compute account on a cluster.\nThe --input flag is used to specify the path to a sample sheet file. This file should be in TSV format and should contain the following columns:\n\nsampleID (required): Unique sample IDs, must start with a letter, and can only contain letters, numbers or underscores\nforwardReads (required): Paths to (forward) reads zipped FastQ files\nreverseReads (optional): Paths to reverse reads zipped FastQ files, required if the data is paired-end\nrun (optional): If the data was produced by multiple sequencing runs, any string\n\nThe --outdir flag is used to specify the path to the output directory.\nAn Ampliseq-compatible sample sheet is generated by the amplicon-multi-cutadapt pipeline.\nSeveral steps in the ampliseq pipeline are not needed if the data will be run through the HAPP pipeline. For instance:\n\n--skip_cutadapt skips the cutadapt step\n--skip_qiime skips the QIIME2 step\n--skip_taxonomy skips the taxonomy step\n--skip_dada_taxonomy skips the dada taxonomy step\n--skip_dada_addspecies skips the dada species assignment step\n\nIf there are samples with too few reads (default threshold is 1) you can set the pipeline to ignore those samples by using the --ignore_empty_input_files and --ignore_failed_trimming",
    "crumbs": [
      "Workflows",
      "Ampliseq pipeline"
    ]
  },
  {
    "objectID": "ampliseq.html#setting-up-the-ampliseq-pipeline",
    "href": "ampliseq.html#setting-up-the-ampliseq-pipeline",
    "title": "Ampliseq pipeline",
    "section": "",
    "text": "Load the nextflow module and the apptainer module\n\nmodule load nextflow\nmodule load PDC apptainer\n\nSet environment variables\n\nexport NXF_HOME=\"&lt;/path/to/where/you/will/run/ampliseq&gt;\"\nexport APPTAINER_BINDPATH=$PDC_TMP\nexport APPTAINER_CACHEDIR=$PDC_TMP/.cache/apptainer\nNote that you will also have to create the $PDC_TMP/.cache/apptainer directory once. You can do so with:\nmkdir -p $PDC_TMP/.cache/apptainer\n\nRun pipeline\n\nThe general syntax to running a Nextflow pipeline is:\nnextflow run &lt;pipeline&gt; -r &lt;release&gt; -profile &lt;profile&gt; --project &lt;project&gt; --input &lt;input&gt; --outdir &lt;outdir&gt;\nArgument flags that begin with a single ‘-’ are Nextflow flags, while those that begin with a double ‘–’ are pipeline-specific flags.\nThe -r flag is used to select a specific release of the pipeline. To see available releases, visit the pipeline’s GitHub page at https://github.com/nf-core/ampliseq/releases. At the time of writing, the latest release is 2.12.0.\nThe -profile flag is used to select a configuration profile for running the pipeline. This can be used to alter the resources allocated to the pipeline, among other things. The Dardel PDC profile for nf-core is available at https://nf-co.re/configs/pdc_kth and can be specified with --profile pdc_kth.\nThe --project flag is used to specify a compute account on a cluster.\nThe --input flag is used to specify the path to a sample sheet file. This file should be in TSV format and should contain the following columns:\n\nsampleID (required): Unique sample IDs, must start with a letter, and can only contain letters, numbers or underscores\nforwardReads (required): Paths to (forward) reads zipped FastQ files\nreverseReads (optional): Paths to reverse reads zipped FastQ files, required if the data is paired-end\nrun (optional): If the data was produced by multiple sequencing runs, any string\n\nThe --outdir flag is used to specify the path to the output directory.\nAn Ampliseq-compatible sample sheet is generated by the amplicon-multi-cutadapt pipeline.\nSeveral steps in the ampliseq pipeline are not needed if the data will be run through the HAPP pipeline. For instance:\n\n--skip_cutadapt skips the cutadapt step\n--skip_qiime skips the QIIME2 step\n--skip_taxonomy skips the taxonomy step\n--skip_dada_taxonomy skips the dada taxonomy step\n--skip_dada_addspecies skips the dada species assignment step\n\nIf there are samples with too few reads (default threshold is 1) you can set the pipeline to ignore those samples by using the --ignore_empty_input_files and --ignore_failed_trimming",
    "crumbs": [
      "Workflows",
      "Ampliseq pipeline"
    ]
  },
  {
    "objectID": "ampliseq.html#example-run",
    "href": "ampliseq.html#example-run",
    "title": "Ampliseq pipeline",
    "section": "Example run",
    "text": "Example run\nnextflow run nf-core/ampliseq -r 2.12.0 -profile pdc_kth --project naiss2024-5-207 \\\n    --input ampliseq_sample_sheet.tsv --outdir results \\\n    --skip_cutadapt --skip_qiime --skip_taxonomy --skip_dada_taxonomy --skip_dada_addspecies --ignore_failed_trimming --ignore_empty_input_files",
    "crumbs": [
      "Workflows",
      "Ampliseq pipeline"
    ]
  },
  {
    "objectID": "ampliseq.html#further-configuration",
    "href": "ampliseq.html#further-configuration",
    "title": "Ampliseq pipeline",
    "section": "Further configuration",
    "text": "Further configuration\nThe PDC profile handles resources etc. for the pipeline on Dardel.\nSteps in the pipeline can be further configured by supplying an additional config file to nextflow. For instance, you can create a file called nextflow.config file and add the following (example for the FASTQC step:\nprocess {\n    withName: 'NFCORE_AMPLISEQ:AMPLISEQ:FASTQC' {\n        time = 10.m\n        cpus = 1\n        memory = 1.GB\n    }\n}\nThen supply the config file to the pipeline:\nnextflow run nf-core/ampliseq -r 2.12.0 -profile pdc_kth -c nextflow.config --project naiss2024-5-207 \\\n    --input ampliseq_sample_sheet.tsv --outdir results \\\n    --skip_cutadapt --skip_qiime --skip_taxonomy --skip_dada_taxonomy --skip_dada_addspecies --ignore_failed_trimming --ignore_empty_input_files",
    "crumbs": [
      "Workflows",
      "Ampliseq pipeline"
    ]
  }
]